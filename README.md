# Optimizers-for-Neural-Network
A brief discussion on some of the variants of Gradient Descent for optimization.

Gradient Descent is an iterative algorithm to find the optimal point for the objective function at hand. In the context of Neural networks, there exist three different variants of Gradient Descent based on the amount of training data used for optimization. These three variants of Gradient Descent see further challenges in their optimization process. We take a look at those challenges and review three of the most widely used techniques by Deep Learning community to tackle those challenges.
